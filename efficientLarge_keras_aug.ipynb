{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from src.augmentations import CustomAugmentations\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "height,width=32,32\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'flip': 0.5, 'gauss_noise': 0.5}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.augmentations.CustomAugmentations import CustomAugmentationsTF\n",
    "\n",
    "# Define the augmentation probabilities\n",
    "p_dict = {\n",
    "  \"flip\": 0.5,\n",
    "  # \"transpose\": 0.5,\n",
    "  \"gauss_noise\": 0.5,\n",
    "  # \"brightness_contrast\": 0.5,\n",
    "  # \"hue_saturation_value\": 0.5,\n",
    "}\n",
    "\n",
    "p_dicts = []\n",
    "# for aug_type in p_dict.keys():\n",
    "#     p_d = {aug_type: .5}\n",
    "#     p_dicts.append(p_d)\n",
    "\n",
    "# p_dict = {\n",
    "#   \"flip\": 0.5,\n",
    "#   \"transpose\": 0.5,\n",
    "#   \"gauss_noise\": 0.5,\n",
    "#   \"brightness_contrast\": 0.5,\n",
    "#   \"hue_saturation_value\": 0.5 ,\n",
    "# }\n",
    "p_dicts.append(p_dict)\n",
    "\n",
    "p_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "augmented = original_dataset.map(lambda image, label: (custom_aug.augment(image), label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da2be3fc4b94641b98f7d8262cdeb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Augmentation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentations: flip0.5_gauss_noise0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baeaade73d2844769f4d088936b3be08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 90000 files belonging to 10 classes.\n",
      "Using 85500 files for training.\n",
      "Found 90000 files belonging to 10 classes.\n",
      "Using 4500 files for validation.\n",
      "Found 90000 files belonging to 10 classes.\n",
      "Fitting model 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50e2f636dd74c9ea5e244178f3c91f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "from src.kerasCNN import *\n",
    "import pickle\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "histories = list()\n",
    "accuracies = list()\n",
    "class_accuracies = list()\n",
    "conf_mats = list()\n",
    "\n",
    "for augs in tqdm(p_dicts, desc=\"Augmentation\"):\n",
    "    aug_str = \"_\".join([key + str(value) for key, value in augs.items()])\n",
    "    print(f\"Augmentations: {aug_str}\")\n",
    "\n",
    "    for iteration in tqdm(range(5), desc=\"Iteration\"):\n",
    "        train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            \"../data/CINIC10/train\",\n",
    "            labels=\"inferred\",\n",
    "            label_mode=\"int\",\n",
    "            class_names=None,\n",
    "            color_mode=\"rgb\",\n",
    "            batch_size=batch_size,\n",
    "            image_size=(height, width),\n",
    "            shuffle=True,\n",
    "            seed=iteration,\n",
    "            validation_split=0.05,\n",
    "            subset=\"training\",\n",
    "        )\n",
    "\n",
    "        val_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            \"../data/CINIC10/valid\",\n",
    "            labels=\"inferred\",\n",
    "            label_mode=\"int\",\n",
    "            class_names=None,\n",
    "            color_mode=\"rgb\",\n",
    "            batch_size=batch_size,\n",
    "            image_size=(height, width),\n",
    "            shuffle=True,\n",
    "            seed=iteration,\n",
    "            validation_split=0.05,\n",
    "            subset=\"validation\",\n",
    "        )\n",
    "\n",
    "        test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            \"../data/CINIC10/test\",\n",
    "            labels=\"inferred\",\n",
    "            label_mode=\"int\",\n",
    "            class_names=None,\n",
    "            color_mode=\"rgb\",\n",
    "            batch_size=batch_size,\n",
    "            image_size=(height, width),\n",
    "            shuffle=False,\n",
    "        )\n",
    "        dnn_model = Sequential()\n",
    "        imported_model = tf.keras.applications.EfficientNetV2L(\n",
    "            include_top=False,\n",
    "            input_shape=(height, width, 3),\n",
    "            pooling=\"max\",\n",
    "            classes=10,\n",
    "            weights=\"imagenet\",\n",
    "        )\n",
    "        dnn_model.add(imported_model)\n",
    "        dnn_model.add(Flatten())\n",
    "        dnn_model.add(Dense(2048, activation=\"relu\"))\n",
    "        dnn_model.add(Dense(1024, activation=\"relu\"))\n",
    "        dnn_model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "        dnn_model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "    )\n",
    "       \n",
    "\n",
    "        history = {\n",
    "            \"loss\": list(),\n",
    "            \"accuracy\": list(),\n",
    "            \"val_loss\": list(),\n",
    "            \"val_accuracy\": list(),\n",
    "        }\n",
    "\n",
    "        print(f\"Fitting model {iteration}\")\n",
    "        for epoch in tqdm(range(EPOCHS), desc=\"Epoch\"):\n",
    "            print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "            custom_aug = CustomAugmentationsTF(augs)\n",
    "            train_set_aug = train_set.map(lambda x, y: (custom_aug.augment(x), y))\n",
    "\n",
    "            hist_ = dnn_model.fit(train_set_aug, validation_data=val_set, epochs=1)\n",
    "            history[\"loss\"].append(hist_.history[\"loss\"])\n",
    "            history[\"accuracy\"].append(hist_.history[\"accuracy\"])\n",
    "            history[\"val_loss\"].append(hist_.history[\"val_loss\"])\n",
    "            history[\"val_accuracy\"].append(hist_.history[\"val_accuracy\"])\n",
    "\n",
    "        print(f\"Predicting model {iteration}\")\n",
    "        preds = dnn_model.predict(test_set, verbose=2)\n",
    "        preds = preds.argmax(axis=1)\n",
    "        classes = test_set.class_names\n",
    "        test_labels = list()\n",
    "        for images, labels in test_set:\n",
    "            class_labels = [int(label) for label in labels]\n",
    "            test_labels.extend(class_labels)\n",
    "        test_labels = np.array(test_labels)\n",
    "\n",
    "        conf_mat = confusion_matrix(test_labels, preds)\n",
    "        accuracy = accuracy_score(test_labels, preds)\n",
    "        print(f\"Accuracy: {accuracy} for model {iteration}\")\n",
    "        class_accuracy = conf_mat.diagonal() / conf_mat.sum(axis=1)\n",
    "\n",
    "        print(f\"Saving model {iteration}\")\n",
    "        dnn_model.save(f\"../models/EfficientNet_aug_{aug_str}_{iteration}.h5\")\n",
    "        histories.append(deepcopy(history))\n",
    "        accuracies.append(deepcopy(accuracy))\n",
    "        class_accuracies.append(deepcopy(class_accuracy))\n",
    "        conf_mats.append(deepcopy(conf_mat))\n",
    "\n",
    "\n",
    "    with open(f\"results/HISTORY_EfficientNet_aug_{aug_str}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(histories, f)\n",
    "\n",
    "    with open(f\"results/ACCURACY_EfficientNet_aug_{aug_str}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(accuracies, f)\n",
    "\n",
    "    with open(f\"results/CLASS_ACCURACY_EfficientNet_aug_{aug_str}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(class_accuracies, f)\n",
    "\n",
    "    with open(f\"results/CONF_MAT_EfficientNet_aug_{aug_str}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(conf_mats, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
