{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from src.augmentations import CustomAugmentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "height,width=32,32\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46871be4ebe8456cb30819254f0d3985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 90000 files belonging to 10 classes.\n",
      "Using 85500 files for training.\n",
      "Found 90000 files belonging to 10 classes.\n",
      "Using 4500 files for validation.\n",
      "Found 90000 files belonging to 10 classes.\n",
      "Fitting model 0\n",
      "Epoch 1/10\n",
      "1336/1336 [==============================] - 192s 126ms/step - loss: 1.2906 - accuracy: 0.5431 - val_loss: 1.2239 - val_accuracy: 0.5871\n",
      "Epoch 2/10\n",
      "1336/1336 [==============================] - 169s 127ms/step - loss: 1.1214 - accuracy: 0.6119 - val_loss: 1.0235 - val_accuracy: 0.6382\n",
      "Epoch 3/10\n",
      "1336/1336 [==============================] - 170s 127ms/step - loss: 0.9796 - accuracy: 0.6592 - val_loss: 0.9815 - val_accuracy: 0.6616\n",
      "Epoch 4/10\n",
      "1336/1336 [==============================] - 166s 124ms/step - loss: 0.9869 - accuracy: 0.6551 - val_loss: 0.9224 - val_accuracy: 0.6784\n",
      "Epoch 5/10\n",
      "1336/1336 [==============================] - 167s 125ms/step - loss: 0.7564 - accuracy: 0.7382 - val_loss: 0.8797 - val_accuracy: 0.7020\n",
      "Epoch 6/10\n",
      "1336/1336 [==============================] - 169s 127ms/step - loss: 0.6288 - accuracy: 0.7842 - val_loss: 0.8956 - val_accuracy: 0.7058\n",
      "Epoch 7/10\n",
      "1336/1336 [==============================] - 170s 127ms/step - loss: 0.5318 - accuracy: 0.8183 - val_loss: 0.9605 - val_accuracy: 0.7056\n",
      "Epoch 8/10\n",
      "1336/1336 [==============================] - 169s 127ms/step - loss: 0.4329 - accuracy: 0.8528 - val_loss: 0.9626 - val_accuracy: 0.7080\n",
      "Epoch 9/10\n",
      "1336/1336 [==============================] - 170s 127ms/step - loss: 0.3509 - accuracy: 0.8827 - val_loss: 1.0418 - val_accuracy: 0.7020\n",
      "Epoch 10/10\n",
      "1336/1336 [==============================] - 169s 127ms/step - loss: 0.2974 - accuracy: 0.9018 - val_loss: 1.0785 - val_accuracy: 0.7013\n",
      "Predicting model 0\n",
      "1407/1407 - 54s - 54s/epoch - 39ms/step\n",
      "Accuracy: 0.7032888888888889 for model 0\n",
      "Saving model 0\n",
      "Found 90000 files belonging to 10 classes.\n",
      "Using 85500 files for training.\n",
      "Found 90000 files belonging to 10 classes.\n",
      "Using 4500 files for validation.\n",
      "Found 90000 files belonging to 10 classes.\n",
      "Fitting model 1\n",
      "Epoch 1/10\n",
      "1336/1336 [==============================] - 192s 128ms/step - loss: 1.2786 - accuracy: 0.5498 - val_loss: 0.9513 - val_accuracy: 0.6720\n",
      "Epoch 2/10\n",
      "1336/1336 [==============================] - 173s 129ms/step - loss: 0.9169 - accuracy: 0.6892 - val_loss: 0.8835 - val_accuracy: 0.6998\n",
      "Epoch 3/10\n",
      "1336/1336 [==============================] - 177s 133ms/step - loss: 1.0630 - accuracy: 0.6321 - val_loss: 0.9832 - val_accuracy: 0.6664\n",
      "Epoch 4/10\n",
      "1336/1336 [==============================] - 170s 127ms/step - loss: 0.9872 - accuracy: 0.6573 - val_loss: 0.9738 - val_accuracy: 0.6567\n",
      "Epoch 5/10\n",
      "1336/1336 [==============================] - 181s 135ms/step - loss: 0.8475 - accuracy: 0.7076 - val_loss: 0.9032 - val_accuracy: 0.6900\n",
      "Epoch 6/10\n",
      "1336/1336 [==============================] - 185s 138ms/step - loss: 0.7405 - accuracy: 0.7441 - val_loss: 0.8054 - val_accuracy: 0.7222\n",
      "Epoch 7/10\n",
      "1336/1336 [==============================] - 184s 137ms/step - loss: 0.5760 - accuracy: 0.8026 - val_loss: 0.8122 - val_accuracy: 0.7258\n",
      "Epoch 8/10\n",
      "1336/1336 [==============================] - 183s 137ms/step - loss: 0.4953 - accuracy: 0.8322 - val_loss: 0.8855 - val_accuracy: 0.7289\n",
      "Epoch 9/10\n",
      "1336/1336 [==============================] - 185s 139ms/step - loss: 0.4202 - accuracy: 0.8573 - val_loss: 0.9239 - val_accuracy: 0.7224\n",
      "Epoch 10/10\n",
      "1336/1336 [==============================] - 187s 140ms/step - loss: 0.3453 - accuracy: 0.8854 - val_loss: 0.9753 - val_accuracy: 0.7324\n",
      "Predicting model 1\n",
      "1407/1407 - 57s - 57s/epoch - 41ms/step\n",
      "Accuracy: 0.7157666666666667 for model 1\n",
      "Saving model 1\n",
      "Found 90000 files belonging to 10 classes.\n",
      "Using 85500 files for training.\n",
      "Found 90000 files belonging to 10 classes.\n",
      "Using 4500 files for validation.\n",
      "Found 90000 files belonging to 10 classes.\n",
      "Fitting model 2\n",
      "Epoch 1/10\n",
      "1336/1336 [==============================] - 203s 137ms/step - loss: 1.2721 - accuracy: 0.5472 - val_loss: 0.9656 - val_accuracy: 0.6587\n",
      "Epoch 2/10\n",
      "1336/1336 [==============================] - 168s 126ms/step - loss: 0.9805 - accuracy: 0.6644 - val_loss: 1.0109 - val_accuracy: 0.6556\n",
      "Epoch 3/10\n",
      "1336/1336 [==============================] - 170s 127ms/step - loss: 0.9101 - accuracy: 0.6860 - val_loss: 0.9231 - val_accuracy: 0.6862\n",
      "Epoch 4/10\n",
      "1336/1336 [==============================] - 188s 141ms/step - loss: 0.7652 - accuracy: 0.7380 - val_loss: 0.9467 - val_accuracy: 0.6756\n",
      "Epoch 5/10\n",
      "1336/1336 [==============================] - 172s 129ms/step - loss: 0.7840 - accuracy: 0.7307 - val_loss: 0.8124 - val_accuracy: 0.7289\n",
      "Epoch 6/10\n",
      "1336/1336 [==============================] - 171s 128ms/step - loss: 0.5795 - accuracy: 0.8026 - val_loss: 0.8623 - val_accuracy: 0.7164\n",
      "Epoch 7/10\n",
      "1336/1336 [==============================] - 186s 139ms/step - loss: 0.5256 - accuracy: 0.8215 - val_loss: 0.8203 - val_accuracy: 0.7362\n",
      "Epoch 8/10\n",
      "1336/1336 [==============================] - 180s 134ms/step - loss: 0.4219 - accuracy: 0.8592 - val_loss: 0.8463 - val_accuracy: 0.7393\n",
      "Epoch 9/10\n",
      "1336/1336 [==============================] - 171s 128ms/step - loss: 0.3500 - accuracy: 0.8841 - val_loss: 0.9734 - val_accuracy: 0.7267\n",
      "Epoch 10/10\n",
      "1336/1336 [==============================] - 171s 128ms/step - loss: 0.2917 - accuracy: 0.9052 - val_loss: 0.9249 - val_accuracy: 0.7393\n",
      "Predicting model 2\n",
      "1407/1407 - 55s - 55s/epoch - 39ms/step\n",
      "Accuracy: 0.7295 for model 2\n",
      "Saving model 2\n",
      "Found 90000 files belonging to 10 classes.\n",
      "Using 85500 files for training.\n",
      "Found 90000 files belonging to 10 classes.\n",
      "Using 4500 files for validation.\n",
      "Found 90000 files belonging to 10 classes.\n",
      "Fitting model 3\n",
      "Epoch 1/10\n",
      "1336/1336 [==============================] - 194s 130ms/step - loss: 1.3042 - accuracy: 0.5368 - val_loss: 1.1177 - val_accuracy: 0.6060\n",
      "Epoch 2/10\n",
      "1336/1336 [==============================] - 176s 132ms/step - loss: 1.0436 - accuracy: 0.6360 - val_loss: 0.9211 - val_accuracy: 0.6753\n",
      "Epoch 3/10\n",
      "1336/1336 [==============================] - 188s 141ms/step - loss: 0.8861 - accuracy: 0.6932 - val_loss: 0.8766 - val_accuracy: 0.6898\n",
      "Epoch 4/10\n",
      "1336/1336 [==============================] - 182s 136ms/step - loss: 0.8160 - accuracy: 0.7191 - val_loss: 0.8036 - val_accuracy: 0.7129\n",
      "Epoch 5/10\n",
      "1336/1336 [==============================] - 171s 128ms/step - loss: 0.6362 - accuracy: 0.7811 - val_loss: 0.8767 - val_accuracy: 0.7189\n",
      "Epoch 6/10\n",
      "1336/1336 [==============================] - 173s 129ms/step - loss: 0.5238 - accuracy: 0.8211 - val_loss: 0.8153 - val_accuracy: 0.7307\n",
      "Epoch 7/10\n",
      "1336/1336 [==============================] - 188s 141ms/step - loss: 0.4274 - accuracy: 0.8541 - val_loss: 0.9680 - val_accuracy: 0.6964\n",
      "Epoch 8/10\n",
      "1336/1336 [==============================] - 189s 141ms/step - loss: 0.3816 - accuracy: 0.8716 - val_loss: 0.9235 - val_accuracy: 0.7187\n",
      "Epoch 9/10\n",
      "1336/1336 [==============================] - 188s 141ms/step - loss: 0.4167 - accuracy: 0.8590 - val_loss: 0.9650 - val_accuracy: 0.7060\n",
      "Epoch 10/10\n",
      "1336/1336 [==============================] - 185s 139ms/step - loss: 0.3061 - accuracy: 0.8973 - val_loss: 1.0217 - val_accuracy: 0.7213\n",
      "Predicting model 3\n",
      "1407/1407 - 58s - 58s/epoch - 41ms/step\n",
      "Accuracy: 0.7215666666666667 for model 3\n",
      "Saving model 3\n",
      "Found 90000 files belonging to 10 classes.\n",
      "Using 85500 files for training.\n",
      "Found 90000 files belonging to 10 classes.\n",
      "Using 4500 files for validation.\n",
      "Found 90000 files belonging to 10 classes.\n",
      "Fitting model 4\n",
      "Epoch 1/10\n",
      "1336/1336 [==============================] - 211s 143ms/step - loss: 1.2761 - accuracy: 0.5495 - val_loss: 1.2161 - val_accuracy: 0.5851\n",
      "Epoch 2/10\n",
      "1336/1336 [==============================] - 188s 141ms/step - loss: 1.1390 - accuracy: 0.6017 - val_loss: 1.0256 - val_accuracy: 0.6458\n",
      "Epoch 3/10\n",
      "1336/1336 [==============================] - 192s 144ms/step - loss: 1.0388 - accuracy: 0.6358 - val_loss: 1.0001 - val_accuracy: 0.6462\n",
      "Epoch 4/10\n",
      "1336/1336 [==============================] - 193s 144ms/step - loss: 1.0021 - accuracy: 0.6477 - val_loss: 0.9716 - val_accuracy: 0.6600\n",
      "Epoch 5/10\n",
      "1336/1336 [==============================] - 193s 144ms/step - loss: 0.9994 - accuracy: 0.6505 - val_loss: 0.9532 - val_accuracy: 0.6682\n",
      "Epoch 6/10\n",
      "1336/1336 [==============================] - 194s 145ms/step - loss: 0.8651 - accuracy: 0.6982 - val_loss: 0.8729 - val_accuracy: 0.6938\n",
      "Epoch 7/10\n",
      "1336/1336 [==============================] - 192s 144ms/step - loss: 0.8656 - accuracy: 0.7007 - val_loss: 0.9054 - val_accuracy: 0.6849\n",
      "Epoch 8/10\n",
      "1336/1336 [==============================] - 194s 145ms/step - loss: 0.7303 - accuracy: 0.7466 - val_loss: 0.8554 - val_accuracy: 0.7047\n",
      "Epoch 9/10\n",
      "1336/1336 [==============================] - 193s 144ms/step - loss: 0.5976 - accuracy: 0.7930 - val_loss: 0.9518 - val_accuracy: 0.6933\n",
      "Epoch 10/10\n",
      "1336/1336 [==============================] - 192s 144ms/step - loss: 0.4932 - accuracy: 0.8303 - val_loss: 1.0391 - val_accuracy: 0.6971\n",
      "Predicting model 4\n",
      "1407/1407 - 59s - 59s/epoch - 42ms/step\n",
      "Accuracy: 0.7073333333333334 for model 4\n",
      "Saving model 4\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "\n",
    "histories = list()\n",
    "accuracies = list()\n",
    "class_accuracies = list()\n",
    "conf_mats = list()\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        \"../data/CINIC10/train\",\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        class_names=None,\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=batch_size,\n",
    "        image_size=(height, width),\n",
    "        shuffle=True,\n",
    "        seed=i,\n",
    "        validation_split=0.05,\n",
    "        subset=\"training\",\n",
    "    )\n",
    "\n",
    "    val_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        \"../data/CINIC10/valid\",\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        class_names=None,\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=batch_size,\n",
    "        image_size=(height, width),\n",
    "        shuffle=True,\n",
    "        seed=i,\n",
    "        validation_split=0.05,\n",
    "        subset=\"validation\",\n",
    "    )\n",
    "\n",
    "    test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        \"../data/CINIC10/test\",\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        class_names=None,\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=batch_size,\n",
    "        image_size=(height, width),\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    dnn_model = Sequential()\n",
    "    imported_model = tf.keras.applications.EfficientNetV2L(\n",
    "        include_top=False,\n",
    "        input_shape=(height, width, 3),\n",
    "        pooling=\"max\",\n",
    "        classes=10,\n",
    "        weights=\"imagenet\",\n",
    "    )\n",
    "\n",
    "    dnn_model.add(imported_model)\n",
    "    dnn_model.add(Flatten())\n",
    "    dnn_model.add(Dense(2048, activation=\"relu\"))\n",
    "    dnn_model.add(Dense(1024, activation=\"relu\"))\n",
    "    dnn_model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    dnn_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    print(f\"Fitting model {i}\")\n",
    "    history = dnn_model.fit(train_set, validation_data=val_set, epochs=10)\n",
    "\n",
    "    print(f\"Predicting model {i}\")\n",
    "    preds = dnn_model.predict(test_set, verbose=2)\n",
    "    preds = preds.argmax(axis=1)\n",
    "    classes = test_set.class_names\n",
    "    test_labels = list()\n",
    "    for images, labels in test_set:\n",
    "        class_labels = [int(label) for label in labels]\n",
    "        test_labels.extend(class_labels)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    conf_mat = confusion_matrix(test_labels, preds)\n",
    "    accuracy = accuracy_score(test_labels, preds)\n",
    "    print(f\"Accuracy: {accuracy} for model {i}\")\n",
    "    class_accuracy = conf_mat.diagonal() / conf_mat.sum(axis=1)\n",
    "\n",
    "    print(f\"Saving model {i}\")\n",
    "    dnn_model.save(f\"../models/EfficientNetV2L_no_aug_{i}.h5\")\n",
    "    histories.append(deepcopy(history.history))\n",
    "    accuracies.append(accuracy)\n",
    "    class_accuracies.append(class_accuracy)\n",
    "    conf_mats.append(conf_mat)\n",
    "\n",
    "\n",
    "# save histories\n",
    "import pickle\n",
    "\n",
    "with open(\"results/HISTORY_EfficientNetV2L_no_aug.pkl\", \"wb\") as f:\n",
    "    pickle.dump(histories, f)\n",
    "\n",
    "# save accuracies\n",
    "with open(\"results/ACCURACY_EfficientNetV2L_no_aug.pkl\", \"wb\") as f:\n",
    "    pickle.dump(accuracies, f)\n",
    "\n",
    "# save class accuracies\n",
    "with open(\"results/CLASS_ACCURACY_EfficientNetV2L_no_aug.pkl\", \"wb\") as f:\n",
    "    pickle.dump(class_accuracies, f)\n",
    "\n",
    "# save confusion matrices\n",
    "with open(\"results/CONF_MAT_EfficientNetV2L_no_aug.pkl\", \"wb\") as f:\n",
    "    pickle.dump(conf_mats, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
